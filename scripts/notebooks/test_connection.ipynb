{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73de11f4",
   "metadata": {},
   "source": [
    "# This notebook test the connection between the compute clusters and datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a482d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Configuration for storage paths, catalog, and schemas\n",
    "CONFIG = {\n",
    "    'catalog_name': 'catalog_name',\n",
    "    'bronze_schema': 'bronze_schema',\n",
    "    'gold_schema': 'gold_schema',\n",
    "    'storage_account': 'datalake name',\n",
    "    'bronze_path': 'abfss://bronze@{storage_account}.dfs.core.windows.net/vitals',\n",
    "    'gold_path': 'abfss://gold@{storage_account}.dfs.core.windows.net/output'\n",
    "}\n",
    "\n",
    "# Format storage paths using the storage account name\n",
    "CONFIG['bronze_path'] = CONFIG['bronze_path'].format(storage_account=CONFIG['storage_account'])\n",
    "CONFIG['gold_path'] = CONFIG['gold_path'].format(storage_account=CONFIG['storage_account'])\n",
    "\n",
    "# Create a Spark session with Unity Catalog enabled\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Azure Data Lake Access\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Set the catalog \n",
    "    print(\"Setting up Unity Catalog context...\")\n",
    "    spark.sql(f\"USE CATALOG {CONFIG['catalog_name']}\")\n",
    "    \n",
    "    # Use bronze schema\n",
    "    print(\"\\nUsing bronze schema for reading...\")\n",
    "    spark.sql(f\"USE SCHEMA {CONFIG['bronze_schema']}\")\n",
    "    \n",
    "    # Read using bronze schema\n",
    "    print(\"Reading data from bronze container...\")\n",
    "    bronze_data = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .json(CONFIG['bronze_path']) \\\n",
    "        .limit(10)\n",
    "\n",
    "    #bronze_data.cache()\n",
    "\n",
    "    print(\"Sample data:\")\n",
    "    bronze_data.show()\n",
    "\n",
    "    # Use gold schema\n",
    "    print(\"\\nUsing gold schema for writing...\")\n",
    "    spark.sql(f\"USE SCHEMA {CONFIG['gold_schema']}\")\n",
    "\n",
    "    # Write using gold schema in delta lake, iceberg, or parquet\n",
    "    bronze_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"{CONFIG['catalog_name']}.{CONFIG['gold_schema']}.patient_data\")\n",
    "   \n",
    "    print(\"Write operation completed successfully\")\n",
    "       \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
